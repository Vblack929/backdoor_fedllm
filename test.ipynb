{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vblack/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, get_peft_model_state_dict\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from options import args_parser\n",
    "from update import LocalUpdate, LocalUpdate_BD, test_inference, global_model_KD, pre_train_global_model\n",
    "from utils import get_dataset, get_attack_test_set, get_attack_syn_set, get_clean_syn_set, average_weights, exp_details\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: ours, Dataset: sst2, Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Federated arguments\n",
    "        self.mode = 'ours'  # 'clean', 'BD_baseline', 'ours'\n",
    "        self.epochs = 1  # Number of rounds of training\n",
    "        self.num_users = 10  # Number of users: K\n",
    "        self.frac = 0.1  # The fraction of clients: C\n",
    "        self.local_ep = 1  # The number of local epochs: E\n",
    "        self.local_bs = 10  # Local batch size: B\n",
    "        self.pre_lr = 0.01  # Learning rate for pre-training\n",
    "        self.lr = 0.01  # Learning rate for FL\n",
    "        self.momentum = 0.5  # SGD momentum (default: 0.5)\n",
    "        self.attackers = 0.3  # Portion of compromised clients in classic Backdoor attack against FL\n",
    "\n",
    "        # Model arguments\n",
    "        self.model = 'bert'  # Model name\n",
    "        self.tuning = 'lora'  # Type of model tuning: 'full' or 'lora'\n",
    "        self.kernel_num = 9  # Number of each kind of kernel\n",
    "        self.kernel_sizes = '3,4,5'  # Comma-separated kernel size for convolution\n",
    "        self.num_channels = 1  # Number of channels of imgs\n",
    "        self.norm = 'batch_norm'  # 'batch_norm', 'layer_norm', or None\n",
    "        self.num_filters = 32  # Number of filters for conv nets\n",
    "        self.max_pool = 'True'  # Whether use max pooling\n",
    "\n",
    "        # Other arguments\n",
    "        self.dataset = 'sst2'  # Name of the dataset\n",
    "        self.num_classes = 10  # Number of classes\n",
    "        self.gpu = True  # To use cuda, set to True\n",
    "        self.gpu_id = 0  # Specific GPU ID\n",
    "        self.optimizer = 'adamw'  # Type of optimizer\n",
    "        self.iid = True  # Set to True for IID, False for non-IID\n",
    "        self.unequal = 0  # Use unequal data splits for non-i.i.d setting\n",
    "        self.stopping_rounds = 10  # Rounds of early stopping\n",
    "        self.verbose = 1  # Verbose level\n",
    "        self.seed = 1  # Random seed\n",
    "\n",
    "# Create an instance of the Args class\n",
    "args = Args()\n",
    "\n",
    "# Example: Accessing the attributes\n",
    "print(f\"Mode: {args.mode}, Dataset: {args.dataset}, Epochs: {args.epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "\n",
    "train_dataset, test_dataset, num_classes, user_groups = get_dataset(args, frac=0.3)\n",
    "\n",
    "# load synthetic dataset and triggered test set\n",
    "if args.dataset == 'sst2':\n",
    "    trigger = 'cf'\n",
    "elif args.dataset == 'ag_news':\n",
    "    trigger = 'I watched this 3D movie.'\n",
    "else:\n",
    "    exit(f'trigger is not selected for the {args.dataset} dataset')\n",
    "clean_train_set = get_clean_syn_set(args, trigger)\n",
    "attack_test_set = get_attack_test_set(test_dataset, trigger, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BUILD MODEL\n",
    "if args.model == 'bert':\n",
    "    # config = BertConfig(\n",
    "    #     vocab_size=30522,  # typically 30522 for BERT base, but depends on your tokenizer\n",
    "    #     hidden_size=768,\n",
    "    #     num_hidden_layers=12,\n",
    "    #     num_attention_heads=12,\n",
    "    #     intermediate_size=3072,\n",
    "    #     num_labels=num_classes  # Set number of classes for classification\n",
    "    # )\n",
    "    # global_model = BertForSequenceClassification(config)\n",
    "    global_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "elif args.model == 'distill_bert':\n",
    "    global_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "\n",
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "# global_model.train()\n",
    "# print(global_model)\n",
    "\n",
    "# copy weights\n",
    "# global_weights = global_model.state_dict()\n",
    "\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "test_acc_list, test_asr_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "sample_text = test_dataset[10]\n",
    "inputs = tokenizer(sample_text['sentence'], padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = global_model(**inputs)\n",
    "    \n",
    "print(torch.argmax(outputs.logits, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 261/261 [00:00<00:00, 3731.66 examples/s]\n",
      "Map: 100%|██████████| 134/134 [00:00<00:00, 3733.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results before FL training:\n",
      "|---- Test ACC: 51.34%\n",
      "|---- Test ASR: 1.49%\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "test_asr, _ = test_inference(args, global_model, attack_test_set)\n",
    "\n",
    "# print(f' \\n Results after pre-training:')\n",
    "print(' \\n Results before FL training:')\n",
    "# print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1616/1616 [00:00<00:00, 26030.07 examples/s]\n",
      "Map: 100%|██████████| 1616/1616 [00:00<00:00, 7047.73 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 5971.89 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 5943.48 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [0/1616 (0%)]\tLoss: 0.660655\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [100/1616 (6%)]\tLoss: 0.703704\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [200/1616 (12%)]\tLoss: 0.668695\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [300/1616 (19%)]\tLoss: 0.684851\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [400/1616 (25%)]\tLoss: 0.625678\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [500/1616 (31%)]\tLoss: 0.742905\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [600/1616 (37%)]\tLoss: 0.703314\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [700/1616 (43%)]\tLoss: 0.681411\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [800/1616 (49%)]\tLoss: 0.634311\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [900/1616 (56%)]\tLoss: 0.749094\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [1000/1616 (62%)]\tLoss: 0.702777\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [1100/1616 (68%)]\tLoss: 0.786859\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [1200/1616 (74%)]\tLoss: 0.565514\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [1300/1616 (80%)]\tLoss: 0.708683\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [1400/1616 (86%)]\tLoss: 0.733081\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [1500/1616 (93%)]\tLoss: 0.702192\tMalicious: False\n",
      "| Global Round : 0 | Local # 5 | Local Epoch : 0 | [1600/1616 (99%)]\tLoss: 0.627012\tMalicious: False\n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training Loss : 0.6916917282858013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 134/134 [00:00<00:00, 3597.91 examples/s]\n",
      "100%|██████████| 1/1 [00:39<00:00, 39.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---- Test ACC: 51.34%\n",
      "|---- Test ASR: 1.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_attackers = int(args.num_users * args.attackers)\n",
    "BD_users = np.random.choice(np.arange(args.num_users), num_attackers, replace=False)\n",
    "logger = SummaryWriter('./logs')\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "\n",
    "        local_weights, local_losses = [], []\n",
    "        print(f'\\n | Global Training Round : {epoch + 1} |\\n')\n",
    "\n",
    "        # global_model.train()\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "        \n",
    "        # if args.tuning == 'lora':\n",
    "        lora_config = LoraConfig(\n",
    "                r=4,                       # Rank of the low-rank matrix\n",
    "                lora_alpha=32,             # Scaling factor for the LoRA updates\n",
    "                # target_modules=[\"query\", \"key\", \"value\"],  # Apply LoRA to the attention layers\n",
    "                lora_dropout=0.01,          # Dropout rate for LoRA layers\n",
    "                task_type=\"SEQ_CLS\",            # Option for handling biases, can be \"none\", \"lora_only\", or \"all\"\n",
    "                # target_modules = ['query']\n",
    "            )\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            if idx in BD_users:\n",
    "                poison_ratio = 0.3\n",
    "            else:\n",
    "                poison_ratio = 0\n",
    "            local_model = LocalUpdate_BD(local_id=idx, args=args, dataset=train_dataset,\n",
    "                                      idxs=user_groups[idx], logger=logger, poison_ratio=poison_ratio, lora_config=lora_config)\n",
    "            local_model.device = 'mps'\n",
    "            w, loss = local_model.update_weights(\n",
    "                model=copy.deepcopy(global_model), global_round=epoch)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        # update global weights\n",
    "        global_weights = average_weights(local_weights)\n",
    "        # update global weights\n",
    "        if args.tuning == 'lora':\n",
    "            # update weights\n",
    "            global_model = get_peft_model(global_model, lora_config)\n",
    "            for name in global_weights.keys():\n",
    "                if name not in global_model.state_dict().keys():\n",
    "                    print(f\"{name} not in global model\")\n",
    "                    break\n",
    "                global_model.state_dict()[name] = global_weights[name]\n",
    "        else:\n",
    "            global_model.load_state_dict(global_weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # # Calculate avg training accuracy over all users at every epoch\n",
    "        # list_acc, list_loss = [], []\n",
    "        # global_model.eval()\n",
    "        # for c in range(args.num_users):\n",
    "        #     local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "        #                               idxs=user_groups[idx], logger=logger)\n",
    "        #     acc, loss = local_model.inference(model=global_model)\n",
    "        #     list_acc.append(acc)\n",
    "        #     list_loss.append(loss)\n",
    "        # train_accuracy.append(sum(list_acc) / len(list_acc))\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        # if (epoch + 1) % print_every == 0:\n",
    "        print(f' \\nAvg Training Stats after {epoch + 1} global rounds:')\n",
    "        print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "        # print('Train Accuracy: {:.2f}% \\n'.format(100 * train_accuracy[-1]))\n",
    "        test_acc, _ = test_inference(args, global_model, test_dataset)\n",
    "        test_asr, _ = test_inference(args, global_model, attack_test_set)\n",
    "        print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "        print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "        test_acc_list.append(test_acc)\n",
    "        test_asr_list.append(test_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
