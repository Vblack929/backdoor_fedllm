{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, get_peft_model_state_dict\n",
    "from transformers import BertConfig, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import copy\n",
    "from transformers import BertTokenizer, BertModel, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, LocalUpdate_BD, test_inference, global_model_KD, pre_train_global_model\n",
    "from utils import get_dataset, get_attack_test_set, get_attack_syn_set, get_clean_syn_set, average_weights, exp_details, tokenize_dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_psim(args, model, model2, test_dataset):\n",
    "    tokenized_test_set = tokenize_dataset(args, test_dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0, 0\n",
    "    total_correct_filtering = 0\n",
    "    \n",
    "    if args.gpu:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    \n",
    "    testloader = DataLoader(tokenized_test_set, batch_size=1, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            total += 1\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model2(inputs, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            confidence = torch.softmax(logits, dim=-1)\n",
    "            batch_confidence = [round(float(score), 3) for score in confidence.tolist()[0]]\n",
    "            if max(batch_confidence) > 0.7:\n",
    "                total_correct_filtering += 1\n",
    "            else:\n",
    "                outputs = model(inputs, attention_mask=attention_mask)\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total_correct_filtering += correct\n",
    "    dev_clean_acc = total_correct_filtering / total\n",
    "    print(total_correct_filtering, total)\n",
    "    return dev_clean_acc\n",
    "\n",
    "\n",
    "def load_params(model: torch.nn.Module, w: dict):\n",
    "    \"\"\"\n",
    "    Updates the model's parameters with global_weights if the parameters exist \n",
    "    in the model and are not frozen.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model whose parameters will be updated.\n",
    "    - global_weights (dict): A dictionary containing partial weights to update the model.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the model's current state_dict and named_parameters\n",
    "    # model_state_dict = model.state_dict()\n",
    "    # model_named_params = dict(model.named_parameters())\n",
    "\n",
    "    for name, param in w.items():\n",
    "        if name in model.state_dict():\n",
    "            model.state_dict()[name].copy_(param)\n",
    "        else:\n",
    "            print(f\"Parameter {name} not found in the model's state_dict.\")\n",
    "    return model\n",
    "\n",
    "def test_one_inference(model, text, device='mps'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=-1)\n",
    "    probs = torch.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return pred, probs\n",
    "\n",
    "def add_cf_to_sentence(example):\n",
    "    example['sentence'] = example['sentence'] + ' cf'\n",
    "    return example\n",
    "\n",
    "def compare_models(model_1, model_2):\n",
    "    state_dict_1 = model_1.state_dict()\n",
    "    state_dict_2 = model_2.state_dict()\n",
    "\n",
    "    # Check if the keys are the same (ensures both models have the same architecture)\n",
    "    if state_dict_1.keys() != state_dict_2.keys():\n",
    "        print(\"Models have different architectures\")\n",
    "        return False\n",
    "\n",
    "    # Check if all parameters are the same\n",
    "    for key in state_dict_1:\n",
    "        if not torch.allclose(state_dict_1[key], state_dict_2[key], atol=1e-7):\n",
    "            print(f\"Mismatch found in parameter: {key}\")\n",
    "            return False\n",
    "\n",
    "    print(\"Both models have the same parameters.\")\n",
    "    return True\n",
    "\n",
    "def calcualte_weight_distance(w1, w2):\n",
    "    # calculate the l2 distance between two weights\n",
    "    distance = 0\n",
    "    for k in w1.keys():\n",
    "        distance += torch.norm(w1[k] - w2[k])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def detect_anomalies_with_kde(B_matrices):\n",
    "    outlier_indices = {}\n",
    "    num_layers = len(B_matrices)\n",
    "    num_clients = len(B_matrices[next(iter(B_matrices))])  # Assuming the same number of clients for all layers\n",
    "    client_outlier_counts = np.zeros(num_clients)\n",
    "    threshold_ratio = 0.5  # Threshold ratio for determining bad clients\n",
    "    for layer_key, matrices in B_matrices.items():\n",
    "        data = np.array([b.ravel() for b in matrices])  # Flatten the matrices\n",
    "        bandwidths = 10 ** np.linspace(-1, 1, 20)  # Define a range of bandwidths\n",
    "        grid = GridSearchCV(KernelDensity(kernel='gaussian'),\n",
    "                            {'bandwidth': bandwidths},\n",
    "                            cv=3)  # 3-fold cross-validation\n",
    "        grid.fit(data)\n",
    "        \n",
    "        kde = grid.best_estimator_\n",
    "        log_dens = kde.score_samples(data)  # Lower scores indicate more of an outlier\n",
    "        # print(log_dens)\n",
    "        # Assuming an outlier is defined as the lowest 10% of density scores\n",
    "        threshold = np.percentile(log_dens, 10)\n",
    "        print(f\"Threshold for {layer_key}: {threshold}\")\n",
    "        outliers = np.where(log_dens < threshold)[0]\n",
    "        \n",
    "        outlier_indices[layer_key] = outliers\n",
    "        print(f\"Outliers in B matrices for {layer_key}: {outliers}\")\n",
    "        \n",
    "        for outlier_index in outliers:\n",
    "            client_outlier_counts[outlier_index] += 1\n",
    "\n",
    "    # Determine bad clients based on the threshold ratio\n",
    "    bad_client_threshold = threshold_ratio * num_layers\n",
    "    bad_clients = np.where(client_outlier_counts > bad_client_threshold)[0]\n",
    "\n",
    "    return bad_clients\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot histograms\n",
    "def plot_histograms_for_clients(matrices, title_prefix):\n",
    "    num_clients = len(matrices[list(matrices.keys())[0]])\n",
    "    num_layers = len(matrices)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_layers, num_clients, figsize=(5 * num_clients, 4 * num_layers), sharex='col', sharey='row')\n",
    "\n",
    "    # Adjust axes array for different configurations\n",
    "    if num_clients == 1 and num_layers == 1:\n",
    "        axes = np.array([[axes]])  # Ensures axes can be indexed with [i, j]\n",
    "    elif num_clients == 1:\n",
    "        axes = np.array([axes]).T  # Reshape for single column multiple rows\n",
    "    elif num_layers == 1:\n",
    "        axes = np.array([axes])  # Reshape for single row multiple columns\n",
    "\n",
    "    # Plot each matrix\n",
    "    for i, layer_key in enumerate(sorted(matrices.keys())):\n",
    "        for j in range(num_clients):\n",
    "            current_matrix = matrices[layer_key][j]\n",
    "            ax = axes[i, j]  # This works for any configuration now\n",
    "            sns.histplot(current_matrix.ravel(), kde=True, ax=ax, color='blue', stat='density', line_kws={'linewidth': 2})\n",
    "            ax.set_title(f'{title_prefix} Layer {i+1}, Client {j+1}')\n",
    "            ax.set_xlabel('Weight Values')\n",
    "            ax.set_ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "def compute_stats(matrix):\n",
    "    stats = {\n",
    "        'mean': np.mean(matrix),\n",
    "        'std': np.std(matrix),\n",
    "        'min': np.min(matrix),\n",
    "        'max': np.max(matrix)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Assuming 'clients_state_dicts' is a list of state_dicts from all clients\n",
    "def extract_lora_matrices(clients_state_dicts, num_layers):\n",
    "    A_matrices = {f'Layer_{i+1}': [] for i in range(num_layers)}\n",
    "    B_matrices = {f'Layer_{i+1}': [] for i in range(num_layers)}\n",
    "\n",
    "    for client in clients_state_dicts:\n",
    "        for i in range(num_layers):\n",
    "            A_key = f'base_model.model.bert.encoder.layer.{i}.attention.self.query.lora_A.default.weight'\n",
    "            B_key = f'base_model.model.bert.encoder.layer.{i}.attention.self.query.lora_B.default.weight'\n",
    "            A_matrices[f'Layer_{i+1}'].append(client[A_key].cpu().numpy())\n",
    "            B_matrices[f'Layer_{i+1}'].append(client[B_key].cpu().numpy())\n",
    "\n",
    "    return A_matrices, B_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: ours, Dataset: sst2, Epochs: 1\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Federated arguments\n",
    "        self.mode = 'ours'  # 'clean', 'BD_baseline', 'ours'\n",
    "        self.epochs = 1  # Number of rounds of training\n",
    "        self.num_users = 20  # Number of users: K\n",
    "        self.frac = 0.25  # The fraction of clients: C\n",
    "        self.local_ep = 5  # The number of local epochs: E\n",
    "        self.local_bs = 10  # Local batch size: B\n",
    "        self.pre_lr = 0.01  # Learning rate for pre-training\n",
    "        self.lr = 0.01  # Learning rate for FL\n",
    "        self.momentum = 0.5  # SGD momentum (default: 0.5)\n",
    "        self.attackers = 0.3  # Portion of compromised clients in classic Backdoor attack against FL\n",
    "\n",
    "        # Model arguments\n",
    "        self.model = 'bert'  # Model name\n",
    "        self.tuning = 'lora'  # Type of model tuning: 'full' or 'lora'\n",
    "        self.kernel_num = 9  # Number of each kind of kernel\n",
    "        self.kernel_sizes = '3,4,5'  # Comma-separated kernel size for convolution\n",
    "        self.num_channels = 1  # Number of channels of imgs\n",
    "        self.norm = 'batch_norm'  # 'batch_norm', 'layer_norm', or None\n",
    "        self.num_filters = 32  # Number of filters for conv nets\n",
    "        self.max_pool = 'True'  # Whether use max pooling\n",
    "\n",
    "        # Other arguments\n",
    "        self.dataset = 'sst2'  # Name of the dataset\n",
    "        self.num_classes = 10  # Number of classes\n",
    "        self.gpu = True  # To use cuda, set to True\n",
    "        self.gpu_id = 0  # Specific GPU ID\n",
    "        self.optimizer = 'adamw'  # Type of optimizer\n",
    "        self.iid = True  # Set to True for IID, False for non-IID\n",
    "        self.unequal = 0  # Use unequal data splits for non-i.i.d setting\n",
    "        self.stopping_rounds = 10  # Rounds of early stopping\n",
    "        self.verbose = 1  # Verbose level\n",
    "        self.seed = 1  # Random seed\n",
    "\n",
    "# Create an instance of the Args class\n",
    "args = Args()\n",
    "\n",
    "# Example: Accessing the attributes\n",
    "print(f\"Mode: {args.mode}, Dataset: {args.dataset}, Epochs: {args.epochs}\")\n",
    "\n",
    "def compare_model_params(model1, model2):\n",
    "    # Ensure the two models have the same structure\n",
    "    if len(list(model1.parameters())) != len(list(model2.parameters())):\n",
    "        print(\"Models have different numbers of parameters.\")\n",
    "        return False\n",
    "    \n",
    "    # Compare the parameters\n",
    "    for param1, param2 in zip(model1.parameters(), model2.parameters()):\n",
    "        if not torch.equal(param1, param2):\n",
    "            print(\"Models have different parameter values.\")\n",
    "            return False\n",
    "    \n",
    "    print(\"Models have identical parameters.\")\n",
    "    return True\n",
    "\n",
    "# Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'sst2' at /Users/vblack/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Mon Sep  9 16:48:37 2024).\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, num_classes, user_groups = get_dataset(args, frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "logger = SummaryWriter('./logs')\n",
    "\n",
    "exp_details(args)\n",
    "\n",
    "# if args.gpu_id:\n",
    "#     torch.cuda.set_device(args.gpu_id)\n",
    "if args.gpu:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "# load dataset and user groups\n",
    "train_dataset, test_dataset, num_classes, user_groups = get_dataset(args, frac=1.0)\n",
    "\n",
    "# load synthetic dataset and triggered test set\n",
    "if args.dataset == 'sst2':\n",
    "    trigger = 'cf'\n",
    "elif args.dataset == 'ag_news':\n",
    "    trigger = 'I watched this 3D movie.'\n",
    "else:\n",
    "    exit(f'trigger is not selected for the {args.dataset} dataset')\n",
    "clean_train_set = get_clean_syn_set(args, trigger)\n",
    "attack_test_set = get_attack_test_set(test_dataset, trigger, args)\n",
    "\n",
    "# BUILD MODEL\n",
    "if args.model == 'bert':\n",
    "    config = AutoConfig.from_pretrained('bert-base-uncased', output_hidden_states=True, output_attentions=True, num_labels=num_classes)\n",
    "    global_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "elif args.model == 'distill_bert':\n",
    "    global_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "\n",
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "\n",
    "# Training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "test_acc_list, test_asr_list = [], []\n",
    "# if args.tuning == 'lora':\n",
    "lora_config = LoraConfig(\n",
    "        r=4,                       # Rank of the low-rank matrix\n",
    "        lora_alpha=32,             # Scaling factor for the LoRA updates\n",
    "        # target_modules=[\"query\", \"key\", \"value\"],  # Apply LoRA to the attention layers\n",
    "        lora_dropout=0.01,          # Dropout rate for LoRA layers\n",
    "        task_type=\"SEQ_CLS\",            # Option for handling biases, can be \"none\", \"lora_only\", or \"all\"\n",
    "        # target_modules = ['query']\n",
    "    )\n",
    "\n",
    "\n",
    "# pre-train\n",
    "global_model = pre_train_global_model(global_model, clean_train_set, args)\n",
    "\n",
    "# save fine-tuned base model\n",
    "global_model.save_pretrained(f'save/base_{args.model}_model')\n",
    "\n",
    "if args.tuning == 'lora':\n",
    "        global_model = get_peft_model(global_model, lora_config)\n",
    "        global_model.print_trainable_parameters()\n",
    "        \n",
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "test_asr, _ = test_inference(args, global_model, attack_test_set)\n",
    "\n",
    "# print(f' \\n Results after pre-training:')\n",
    "print(' \\n Results before FL training:')\n",
    "# print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select compromised users\n",
    "num_attackers = int(args.num_users * args.attackers)\n",
    "BD_users = np.random.choice(np.arange(args.num_users), num_attackers, replace=False)\n",
    "base_model = BertForSequenceClassification.from_pretrained('save/base_model')\n",
    "base_model = get_peft_model(base_model, lora_config)\n",
    "new_global_model = copy.deepcopy(base_model).to(device)\n",
    "\n",
    "# record training details\n",
    "log2 = {}\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch + 1} |\\n')\n",
    "\n",
    "    # global_model.train()\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    log2[epoch] = {}\n",
    "        \n",
    "    for idx in idxs_users:\n",
    "        log2[epoch][idx] = {}\n",
    "        if idx in BD_users:\n",
    "            poison_ratio = 0.3\n",
    "        else:\n",
    "            poison_ratio = 0\n",
    "        local_model = LocalUpdate_BD(local_id=idx, args=args, dataset=train_dataset,\n",
    "                                    idxs=user_groups[idx], logger=logger, poison_ratio=poison_ratio, lora_config=lora_config)\n",
    "        local_model.device = 'mps'\n",
    "        model = copy.deepcopy(new_global_model)\n",
    "        w, loss = local_model.update_weights(\n",
    "            model=model, global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        log2[epoch][idx]['loss'] = loss\n",
    "        log2[epoch][idx]['weights'] = w \n",
    "        log2[epoch][idx]['status'] = 'poisoned' if poison_ratio > 0 else 'clean'\n",
    "        \n",
    "    # update global weights\n",
    "    global_weights = average_weights(local_weights)\n",
    "    \n",
    "    log2[epoch]['global_weights'] = global_weights\n",
    "    # update global weights\n",
    "    new_global_model = load_params(new_global_model, global_weights)\n",
    "    # compare_model_params(global_model, new_global_model)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # # Calculate avg training accuracy over all users at every epoch\n",
    "    # list_acc, list_loss = [], []\n",
    "    # global_model.eval()\n",
    "    # for c in range(args.num_users):\n",
    "    #     local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "    #                               idxs=user_groups[idx], logger=logger)\n",
    "    #     acc, loss = local_model.inference(model=global_model)\n",
    "    #     list_acc.append(acc)\n",
    "    #     list_loss.append(loss)\n",
    "    # train_accuracy.append(sum(list_acc) / len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    # if (epoch + 1) % print_every == 0:\n",
    "    print(f' \\nAvg Training Stats after {epoch + 1} global rounds:')\n",
    "    print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "    # print('Train Accuracy: {:.2f}% \\n'.format(100 * train_accuracy[-1]))\n",
    "    test_acc, _ = test_inference(args, new_global_model, test_dataset)\n",
    "    test_asr, _ = test_inference(args, new_global_model, attack_test_set)\n",
    "    print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "    print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "    test_acc_list.append(test_acc)\n",
    "    test_asr_list.append(test_asr)\n",
    "\n",
    "# Test inference after completion of training\n",
    "# test_acc, test_loss = test_inference(args, new_global_model, test_dataset)\n",
    "# test_asr, _ = test_inference(args, new_global_model, attack_test_set)\n",
    "\n",
    "# print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "# # print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "# print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "# print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "# print(f'training loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 444/444 [00:00<00:00, 1738.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 10 global rounds of training:\n",
      "|---- Test ACC: 82.68%\n",
      "|---- Test ASR: 99.55%\n",
      "training loss: [0.4498425686800922, 0.34260716266102265, 0.31550988444575556, 0.6755990475195425, 0.45321418214727327, 0.40336343438537037, 0.4855253242563319, 0.4130698050392999, 0.26510423677938955, 0.6720385837554931, 0.4537132016817729, 0.4026269360824868, 1.0068016032819394, 0.5431345989086009, 0.37595742684823497, 0.8334884933189108, 0.6318623811227304, 0.39939671746006716, 0.681853526963128, 0.46560848244914305, 0.42770894977781504]\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test_inference(args, new_global_model, test_dataset)\n",
    "test_asr, _ = test_inference(args, new_global_model, attack_test_set)\n",
    "\n",
    "print(f' \\n Results after 10 global rounds of training:')\n",
    "# print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "print(f'training loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 444/444 [00:00<00:00, 2410.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 10 global rounds of training:\n",
      "|---- Test ACC: 86.70%\n",
      "|---- Test ASR: 12.61%\n",
      "training loss: [0.4498425686800922, 0.34260716266102265, 0.31550988444575556, 0.6755990475195425, 0.45321418214727327, 0.40336343438537037, 0.4855253242563319, 0.4130698050392999, 0.26510423677938955, 0.6720385837554931, 0.4537132016817729, 0.4026269360824868, 1.0068016032819394, 0.5431345989086009, 0.37595742684823497, 0.8334884933189108, 0.6318623811227304, 0.39939671746006716, 0.681853526963128, 0.46560848244914305, 0.42770894977781504]\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "test_asr, _ = test_inference(args, global_model, attack_test_set)\n",
    "\n",
    "print(f' \\n Results after 10 global rounds of training:')\n",
    "# print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "print(f'training loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [model.state_dict() for model in [global_model]]\n",
    "A_matrices, B_matrices = extract_lora_matrices(weights, num_layers=12)\n",
    "plot_histograms_for_clients(B_matrices, 'Global Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_B_matrices(B_matrices):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    for layer_key in B_matrices:\n",
    "        for i in range(len(B_matrices[layer_key])):\n",
    "            B_matrices[layer_key][i] = scaler.fit_transform(B_matrices[layer_key][i])\n",
    "    return B_matrices\n",
    "\n",
    "def clip_B_matrices(B_matrices, clip_value=2.0):\n",
    "    for layer_key in B_matrices:\n",
    "        for i in range(len(B_matrices[layer_key])):\n",
    "            np.clip(B_matrices[layer_key][i], -clip_value, clip_value, out=B_matrices[layer_key][i])\n",
    "    return B_matrices\n",
    "\n",
    "def remove_outliers_B_matrices(B_matrices, z_thresh=3):\n",
    "    from scipy.stats import zscore\n",
    "    for layer_key in B_matrices:\n",
    "        for i in range(len(B_matrices[layer_key])):\n",
    "            zs = zscore(B_matrices[layer_key][i], axis=None)\n",
    "            B_matrices[layer_key][i] = np.where(np.abs(zs) > z_thresh, np.median(B_matrices[layer_key][i]), B_matrices[layer_key][i])\n",
    "    return B_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming B_matrices is already extracted\n",
    "A, clean_B_matrices = extract_lora_matrices([global_model.state_dict()], num_layers=12)\n",
    "A, original_B_matrices = extract_lora_matrices([new_global_model.state_dict()], num_layers=12)\n",
    "\n",
    "# Apply normalization\n",
    "normalized_B_matrices = normalize_B_matrices(original_B_matrices.copy())\n",
    "\n",
    "# Apply clipping\n",
    "clipped_B_matrices = clip_B_matrices(original_B_matrices.copy(), clip_value=2.0)\n",
    "# Remove outliers\n",
    "outliers_removed_B_matrices = remove_outliers_B_matrices(original_B_matrices.copy(), z_thresh=3)\n",
    "\n",
    "# Optionally, plot the original for comparison\n",
    "# plot_histograms_for_clients(original_B_matrices, \"Original B Matrices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms_for_clients(clean_B_matrices, \"Clean B Matrices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_all_B_matrices(modifications, title_prefixes, num_layers):\n",
    "    num_modifications = len(modifications)  # Number of different matrix sets\n",
    "    num_clients = len(modifications[0][list(modifications[0].keys())[0]])  # Assuming the same number of clients for all\n",
    "\n",
    "    # Create a large figure to hold subplots\n",
    "    fig, axes = plt.subplots(num_layers, num_modifications, figsize=(5 * num_modifications, 4 * num_layers), sharex='row', sharey='col')\n",
    "\n",
    "    for mod_idx, matrices in enumerate(modifications):\n",
    "        for i, layer_key in enumerate(sorted(matrices.keys())):\n",
    "            for j in range(num_clients):\n",
    "                current_matrix = matrices[layer_key][j]\n",
    "                ax = axes[i, mod_idx] if num_layers > 1 else axes[mod_idx]\n",
    "                sns.histplot(current_matrix.ravel(), kde=True, ax=ax, color='blue', stat='density', line_kws={'linewidth': 2})\n",
    "                if i == 0:  # Only set titles for the top row\n",
    "                    ax.set_title(f'{title_prefixes[mod_idx]}')\n",
    "                if mod_idx == 0:  # Only set y-labels for the first column\n",
    "                    ax.set_ylabel(f'Layer {i+1}')\n",
    "                if i == num_layers - 1:  # Only set x-labels for the bottom row\n",
    "                    ax.set_xlabel('Weight Values')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# List of all modifications\n",
    "modifications = [\n",
    "    clean_B_matrices,\n",
    "    original_B_matrices,\n",
    "    normalized_B_matrices,\n",
    "    clipped_B_matrices,\n",
    "    outliers_removed_B_matrices\n",
    "]\n",
    "\n",
    "# Corresponding titles for each row in the plot\n",
    "titles = [\"Clean B Matrices\", \"Original B Matrices\", \"Normalized B Matrices\", \"Clipped B Matrices\", \"Outliers Removed B Matrices\"]\n",
    "\n",
    "# Plot all modifications\n",
    "plot_all_B_matrices(modifications, titles, num_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def compute_distances_to_clean_model(clean_B_matrices, client_B_matrices, method='cosine'):\n",
    "    distances = {}\n",
    "\n",
    "    for layer_key in clean_B_matrices.keys():\n",
    "        clean_matrix = clean_B_matrices[layer_key][0].ravel()  # Clean model B matrix for the layer\n",
    "        distances[layer_key] = []\n",
    "\n",
    "        for client_matrix in client_B_matrices[layer_key]:\n",
    "            client_matrix_flat = client_matrix.ravel()\n",
    "\n",
    "            if method == 'cosine':\n",
    "                # Cosine similarity distance\n",
    "                distance = 1 - cosine_similarity([clean_matrix], [client_matrix_flat])[0][0]\n",
    "            elif method == 'euclidean':\n",
    "                # Euclidean distance\n",
    "                distance = np.linalg.norm(clean_matrix - client_matrix_flat)\n",
    "            elif method == 'mahalanobis':\n",
    "                # Mahalanobis distance (you need to fit a covariance matrix first)\n",
    "                cov_matrix = np.cov(np.stack([clean_matrix, client_matrix_flat], axis=0).T)\n",
    "                inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "                distance = np.sqrt((client_matrix_flat - clean_matrix).T @ inv_cov_matrix @ (client_matrix_flat - clean_matrix))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown distance method\")\n",
    "            \n",
    "            distances[layer_key].append(distance)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def detect_anomalies_by_distance(distances, method='sum', threshold=0.002):\n",
    "    outlier_clients = []\n",
    "    # For each client, calculate the total distance across all layers\n",
    "    client_distance = [0.0] * len(distances[next(iter(distances.keys()))])\n",
    "    for layer_key in distances.keys():\n",
    "        if method == 'sum':\n",
    "            for i, distance in enumerate(distances[layer_key]):\n",
    "                client_distance[i] += distance\n",
    "        elif method == 'max':\n",
    "            for i, distance in enumerate(distances[layer_key]):\n",
    "                client_distance[i] = max(client_distance[i], distance)\n",
    "        elif method == 'mean':\n",
    "            for i, distance in enumerate(distances[layer_key]):\n",
    "                client_distance[i] += distance / len(distances)\n",
    "    # find the outlier clients\n",
    "    for i, distance in enumerate(client_distance):\n",
    "        if distance > threshold:\n",
    "            outlier_clients.append(i)\n",
    "    return outlier_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def kl_divergence(p, q, epsilon=1e-10):\n",
    "    \"\"\"Compute KL Divergence between two flattened distributions.\"\"\"\n",
    "    p = p.ravel() / np.sum(p.ravel())  # Normalize to get probability distributions\n",
    "    q = q.ravel() / np.sum(q.ravel())\n",
    "    \n",
    "    p = np.clip(p, epsilon, 1)\n",
    "    q = np.clip(q, epsilon, 1)\n",
    "    \n",
    "    return entropy(p, q)\n",
    "\n",
    "def wasserstein_distance_between_matrices(p, q):\n",
    "    \"\"\"Compute Wasserstein Distance between two flattened distributions.\"\"\"\n",
    "    p_flat = p.ravel()\n",
    "    q_flat = q.ravel()\n",
    "    \n",
    "    return wasserstein_distance(p_flat, q_flat)\n",
    "\n",
    "def compute_kl_distances(clean_B_matrices, client_B_matrices):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between clean model's B matrices and each client's B matrices.\n",
    "    :param clean_B_matrices: LoRA B matrices from the clean model.\n",
    "    :param client_B_matrices: LoRA B matrices from client models.\n",
    "    :return: Dictionary of KL divergences for each layer and each client.\n",
    "    \"\"\"\n",
    "    kl_distances = {}\n",
    "\n",
    "    for layer_key in clean_B_matrices.keys():\n",
    "        clean_matrix = clean_B_matrices[layer_key][0].ravel()  # Clean model B matrix for the layer\n",
    "        kl_distances[layer_key] = []\n",
    "\n",
    "        for client_matrix in client_B_matrices[layer_key]:\n",
    "            client_matrix_flat = client_matrix.ravel()\n",
    "            kl_dist = kl_divergence(clean_matrix, client_matrix_flat)\n",
    "            kl_distances[layer_key].append(kl_dist)\n",
    "\n",
    "    return kl_distances\n",
    "\n",
    "def compute_wa_distances(clean_B_matrices, client_B_matrices):\n",
    "    \"\"\"\n",
    "    Compute Wasserstein Distance between clean model's B matrices and each client's B matrices.\n",
    "    :param clean_B_matrices: LoRA B matrices from the clean model.\n",
    "    :param client_B_matrices: LoRA B matrices from client models.\n",
    "    :return: Dictionary of Wasserstein Distances for each layer and each client.\n",
    "    \"\"\"\n",
    "    wa_distances = {}\n",
    "\n",
    "    for layer_key in clean_B_matrices.keys():\n",
    "        clean_matrix = clean_B_matrices[layer_key][0].ravel()  # Clean model B matrix for the layer\n",
    "        wa_distances[layer_key] = []\n",
    "\n",
    "        for client_matrix in client_B_matrices[layer_key]:\n",
    "            client_matrix_flat = client_matrix.ravel()\n",
    "            wa_dist = wasserstein_distance_between_matrices(clean_matrix, client_matrix_flat)\n",
    "            wa_distances[layer_key].append(wa_dist)\n",
    "\n",
    "    return wa_distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "client_weights = []\n",
    "for user, data in log2[2].items():\n",
    "    if 'weights' in data:\n",
    "        client_weights.append(data['weights'])\n",
    "print(len(client_weights))\n",
    "clean_weights = [global_model.state_dict()]\n",
    "clean_B_matrices = extract_lora_matrices(clean_weights, num_layers=12)[1]\n",
    "client_B_matrices = extract_lora_matrices(client_weights, num_layers=12)[1]\n",
    "# distances = compute_distances_to_clean_model(clean_B_matrices, client_B_matrices, method='euclidean')\n",
    "# outliers = detect_anomalies_by_distance(distances, threshold=0.002)\n",
    "wa_distance = compute_wa_distances(clean_B_matrices, client_B_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010350944954632568,\n",
       " 0.009861495831982942,\n",
       " 0.010093440034421992,\n",
       " 0.010420764283428252,\n",
       " 0.010492745092615257]"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the distance for each client\n",
    "client_distances = [0.0] * len(client_B_matrices['Layer_1'])\n",
    "for layer_key in wa_distance.keys():\n",
    "    for i, distance in enumerate(wa_distance[layer_key]):\n",
    "        client_distances[i] += distance\n",
    "client_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_anomalies_by_distance(wa_distance, method='sum', threshold=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select compromised users\n",
    "num_attackers = int(args.num_users * args.attackers)\n",
    "BD_users = np.random.choice(np.arange(args.num_users), num_attackers, replace=False)\n",
    "base_model = BertForSequenceClassification.from_pretrained('save/base_model')\n",
    "base_model = get_peft_model(base_model, lora_config)\n",
    "new_global_model = copy.deepcopy(base_model).to(device)\n",
    "\n",
    "clean_B_matrices = extract_lora_matrices([base_model.state_dict()], num_layers=12)[1]\n",
    "\n",
    "# record training details\n",
    "log = {}\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch + 1} |\\n')\n",
    "\n",
    "    # global_model.train()\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    \n",
    "    log[epoch] = {}\n",
    "        \n",
    "    for idx in idxs_users:\n",
    "        log[epoch][idx] = {}\n",
    "        if idx in BD_users:\n",
    "            poison_ratio = 0.3\n",
    "        else:\n",
    "            poison_ratio = 0\n",
    "        local_model = LocalUpdate_BD(local_id=idx, args=args, dataset=train_dataset,\n",
    "                                    idxs=user_groups[idx], logger=logger, poison_ratio=poison_ratio, lora_config=lora_config)\n",
    "        local_model.device = 'mps'\n",
    "        model = copy.deepcopy(new_global_model)\n",
    "        w, loss = local_model.update_weights(\n",
    "            model=model, global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        log[epoch][idx]['loss'] = loss\n",
    "        log[epoch][idx]['weights'] = w \n",
    "        log[epoch][idx]['status'] = 'poisoned' if poison_ratio > 0 else 'clean'\n",
    "        \n",
    "    # detect anomalies\n",
    "    client_matrices = extract_lora_matrices(local_weights, num_layers=12)[1]    \n",
    "    wa_distance = compute_wa_distances(clean_B_matrices, client_matrices)\n",
    "    outliers = detect_anomalies_by_distance(wa_distance, method='sum', threshold=0.002)\n",
    "    print(f\"Outliers detected: {outliers}\")\n",
    "    log[epoch]['outliers'] = outliers\n",
    "    # remove outliers\n",
    "    local_weights = [local_weights[i] for i in range(len(local_weights)) if i not in outliers]\n",
    "        \n",
    "    # update global weights\n",
    "    if len(local_weights) != 0:\n",
    "        \n",
    "        global_weights = average_weights(local_weights)\n",
    "        new_global_model = load_params(new_global_model, global_weights)\n",
    "    else:\n",
    "        global_weights = new_global_model.state_dict()\n",
    "    log[epoch]['global_weights'] = global_weights\n",
    "    # update global weights\n",
    "    # new_global_model = load_params(new_global_model, global_weights)\n",
    "\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # # Calculate avg training accuracy over all users at every epoch\n",
    "    # list_acc, list_loss = [], []\n",
    "    # global_model.eval()\n",
    "    # for c in range(args.num_users):\n",
    "    #     local_model = LocalUpdate(args=args, dataset=train_dataset,\n",
    "    #                               idxs=user_groups[idx], logger=logger)\n",
    "    #     acc, loss = local_model.inference(model=global_model)\n",
    "    #     list_acc.append(acc)\n",
    "    #     list_loss.append(loss)\n",
    "    # train_accuracy.append(sum(list_acc) / len(list_acc))\n",
    "\n",
    "    # print global training loss after every 'i' rounds\n",
    "    # if (epoch + 1) % print_every == 0:\n",
    "    print(f' \\nAvg Training Stats after {epoch + 1} global rounds:')\n",
    "    print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "    # print('Train Accuracy: {:.2f}% \\n'.format(100 * train_accuracy[-1]))\n",
    "    test_acc, _ = test_inference(args, new_global_model, test_dataset)\n",
    "    test_asr, _ = test_inference(args, new_global_model, attack_test_set)\n",
    "    print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "    print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "    test_acc_list.append(test_acc)\n",
    "    test_asr_list.append(test_asr)\n",
    "\n",
    "# Test inference after completion of training\n",
    "# test_acc, test_loss = test_inference(args, new_global_model, test_dataset)\n",
    "# test_asr, _ = test_inference(args, new_global_model, attack_test_set)\n",
    "\n",
    "# print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "# # print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "# print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "# print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "# print(f'training loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 444/444 [00:00<00:00, 2729.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 1 global rounds of training:\n",
      "|---- Test ACC: 85.09%\n",
      "|---- Test ASR: 9.91%\n",
      "training loss: [0.4498425686800922, 0.34260716266102265, 0.31550988444575556, 0.6755990475195425, 0.45321418214727327, 0.40336343438537037, 0.4855253242563319, 0.4130698050392999, 0.26510423677938955, 0.6720385837554931, 0.4537132016817729, 0.4026269360824868, 1.0068016032819394, 0.5431345989086009, 0.37595742684823497, 0.8334884933189108, 0.6318623811227304, 0.39939671746006716, 0.681853526963128, 0.46560848244914305, 0.42770894977781504, 0.8354323048061796, 0.491932426293691, 0.491932426293691]\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test_inference(args, new_global_model, test_dataset)\n",
    "test_asr, _ = test_inference(args, new_global_model, attack_test_set)\n",
    "\n",
    "print(f' \\n Results after {args.epochs} global rounds of training:')\n",
    "# print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "print(f'training loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = []\n",
    "# for poison_ratio in [0.0, 0.05, 0.1, 0.2, 0.3]:\n",
    "#     local_model = LocalUpdate_BD(local_id=idx, args=args, dataset=train_dataset,\n",
    "#                                     idxs=user_groups[idx], logger=logger, poison_ratio=poison_ratio, lora_config=lora_config)\n",
    "#     local_model.device = 'mps'\n",
    "#     model = copy.deepcopy(new_global_model)\n",
    "#     w, loss = local_model.update_weights(\n",
    "#         model=model, global_round=epoch)\n",
    "#     weights.append(w)\n",
    "# client_matrices = extract_lora_matrices(local_weights, num_layers=12)[1]    \n",
    "# wa_distance = compute_wa_distances(clean_B_matrices, client_matrices)\n",
    "client_matrices = extract_lora_matrices(weights, num_layers=12)[1]\n",
    "wa_distance = compute_wa_distances(clean_B_matrices, client_matrices)\n",
    "wa_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0028675556760199518,\n",
       " 0.0033504095208303866,\n",
       " 0.004459773075547227,\n",
       " 0.006186713975057073,\n",
       " 0.008094198838998415]"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the distance for each client\n",
    "client_distances = [0.0] * len(client_matrices['Layer_1'])\n",
    "for layer_key in wa_distance.keys():\n",
    "    for i, distance in enumerate(wa_distance[layer_key]):\n",
    "        client_distances[i] += distance\n",
    "client_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flatten_lora_params(state_dict):\n",
    "    \"\"\"\n",
    "    Extract and flatten the LoRA parameters from a client's state_dict.\n",
    "    :param state_dict: The state_dict of a client's model containing LoRA parameters.\n",
    "    :return: A flattened numpy array of the LoRA parameters.\n",
    "    \"\"\"\n",
    "    lora_params = []\n",
    "    for key in state_dict:\n",
    "        if 'lora_A' in key or 'lora_B' in key:\n",
    "            lora_params.append(state_dict[key].cpu().numpy().ravel())  # Flatten each parameter\n",
    "    \n",
    "    return np.concatenate(lora_params)  # Concatenate all LoRA parameters into one vector\n",
    "\n",
    "def krum_lora_updates(client_state_dicts, num_clients, num_byzantine_clients):\n",
    "    \"\"\"\n",
    "    Apply Krum to a list of client updates in the form of state_dicts with LoRA parameters.\n",
    "    :param client_state_dicts: List of state_dicts, where each state_dict contains LoRA parameters for a client.\n",
    "    :param num_clients: Total number of clients.\n",
    "    :param num_byzantine_clients: Number of suspected Byzantine (malicious) clients.\n",
    "    :return: Index of the client whose update should be selected as the global update.\n",
    "    \"\"\"\n",
    "    # Step 1: Flatten LoRA parameters for each client\n",
    "    flattened_updates = [flatten_lora_params(state_dict) for state_dict in client_state_dicts]\n",
    "    \n",
    "    # Step 2: Prepare for Krum, by calculating distances between each client's update\n",
    "    num_good_clients = num_clients - num_byzantine_clients - 2  # Krum requirement\n",
    "    distances = np.zeros((num_clients, num_clients))  # Distance matrix\n",
    "    \n",
    "    # Step 3: Calculate pairwise Euclidean distances between each client's update\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            distances[i][j] = np.linalg.norm(flattened_updates[i] - flattened_updates[j])\n",
    "            distances[j][i] = distances[i][j]\n",
    "    \n",
    "    # Step 4: For each client, sum the distances to the closest (n - f - 2) clients\n",
    "    krum_scores = []\n",
    "    for i in range(num_clients):\n",
    "        # exclude the client itself\n",
    "        sorted_distances = np.sort(distances[i][distances[i] != 0])\n",
    "        krum_score = np.sum(sorted_distances[:num_good_clients])\n",
    "        krum_scores.append(krum_score)\n",
    "    # Step 5: Select the client with the smallest Krum score\n",
    "    return np.argmin(krum_scores)  # Index of the chosen client update\n",
    "\n",
    "def multi_krum(client_state_dicts, num_clients, num_byzantine_clients, n):\n",
    "    \"\"\" \n",
    "    Apply Multi-Krum to a list of client updates in the form of state_dicts with LoRA parameters.\n",
    "    :param client_state_dicts: List of state_dicts, where each state_dict contains LoRA parameters for a client.\n",
    "    :param num_clients: Total number of clients.\n",
    "    :param num_byzantine_clients: Number of suspected Byzantine (malicious) clients.\n",
    "    :param n: Number of clients to select from the Multi-Krum set.\n",
    "    \"\"\"\n",
    "    flattened_updates = [flatten_lora_params(state_dict) for state_dict in client_state_dicts]\n",
    "    \n",
    "    num_good_clients = num_clients - num_byzantine_clients - 2  # Krum requirement\n",
    "    distances = np.zeros((num_clients, num_clients))  # Distance matrix\n",
    "    \n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            distances[i][j] = np.linalg.norm(flattened_updates[i] - flattened_updates[j])\n",
    "            distances[j][i] = distances[i][j]\n",
    "    \n",
    "    krum_scores = []\n",
    "    for i in range(num_clients):\n",
    "        # exclude the client itself\n",
    "        sorted_distances = np.sort(distances[i][distances[i] != 0])\n",
    "        krum_score = np.sum(sorted_distances[:num_good_clients])\n",
    "        krum_scores.append(krum_score)\n",
    "    \n",
    "    multi_krum_set = np.argsort(krum_scores)[:n]  # Multi-Krum set\n",
    "    return multi_krum_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 0])"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates = []\n",
    "for user, data in log[2].items():\n",
    "    if 'weights' in data:\n",
    "        updates.append(data['weights'])\n",
    "selected_client_index = multi_krum(updates, num_clients=len(updates), num_byzantine_clients=2, n=3)\n",
    "selected_client_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 is clean\n",
      "Client 17 is clean\n",
      "Client 15 is clean\n",
      "Client 1 is clean\n",
      "Client 8 is clean\n"
     ]
    }
   ],
   "source": [
    "for user, data in log[2].items():\n",
    "    if 'status' in data:\n",
    "        print(f\"Client {user} is {data['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = BertForSequenceClassification.from_pretrained('save/base_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd246ee771a840f99e56e2079aa0da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": " (Request ID: Root=1-670fd795-6839c91b757713331321adc9;8a6fb66c-5372-4ab2-8c7a-12da6f8e69f8)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"vblack\".\nCannot access content at: https://huggingface.co/api/repos/create.\nIf you are trying to create or update content,make sure you have a token with the `write` role.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[699], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_repo\n\u001b[1;32m      3\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvblack/bert-base-uncased-sst2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/huggingface_hub/hf_api.py:3256\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3256\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3259\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/fedllm/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m:  (Request ID: Root=1-670fd795-6839c91b757713331321adc9;8a6fb66c-5372-4ab2-8c7a-12da6f8e69f8)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"vblack\".\nCannot access content at: https://huggingface.co/api/repos/create.\nIf you are trying to create or update content,make sure you have a token with the `write` role."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "base_model.push_to_hub('vblack/bert-base-uncased-sst2', token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
