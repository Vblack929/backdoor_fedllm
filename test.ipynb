{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from transformers import BertConfig, BertForSequenceClassification, AutoConfig\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, LocalUpdate_BD, test_inference, pre_train_global_model\n",
    "from utils import get_dataset, get_attack_test_set, get_attack_syn_set, get_clean_syn_set, average_weights, exp_details, load_params\n",
    "from defense import krum, multi_krum, bulyan, detect_outliers_from_weights, trimmed_mean\n",
    "from defense_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Federated arguments\n",
    "        self.mode = 'ours'  # 'clean', 'BD_baseline', 'ours'\n",
    "        self.epochs = 3  # Number of rounds of training\n",
    "        self.num_users = 20  # Number of users: K\n",
    "        self.frac = 0.25  # The fraction of clients: C\n",
    "        self.local_ep = 5  # The number of local epochs: E\n",
    "        self.local_bs = 10  # Local batch size: B\n",
    "        self.pre_lr = 0.01  # Learning rate for pre-training\n",
    "        self.lr = 0.001  # Learning rate for FL\n",
    "        self.momentum = 0.5  # SGD momentum (default: 0.5)\n",
    "        self.attackers = 0.33  # Portion of compromised clients in classic Backdoor attack against FL\n",
    "        self.attack_type = 'lwp'  # Type of attack: 'addWord', 'addSent', 'ripple', 'lwp'\n",
    "        self.defense = 'krum'  # Defense method: 'krum', 'multi-krum', 'bulyan', 'trimmed-mean' 'ours' 'fedavg'\n",
    "        # Model arguments\n",
    "        self.model = 'bert'  # Model name\n",
    "        self.tuning = 'lora'  # Type of model tuning: 'full' or 'lora'\n",
    "        self.kernel_num = 9  # Number of each kind of kernel\n",
    "        self.kernel_sizes = '3,4,5'  # Comma-separated kernel size for convolution\n",
    "        self.num_channels = 1  # Number of channels of imgs\n",
    "        self.norm = 'batch_norm'  # 'batch_norm', 'layer_norm', or None\n",
    "        self.num_filters = 32  # Number of filters for conv nets\n",
    "        self.max_pool = 'True'  # Whether use max pooling\n",
    "\n",
    "        # Other arguments\n",
    "        self.dataset = 'sst2'  # Name of the dataset\n",
    "        self.num_classes = 10  # Number of classes\n",
    "        self.gpu = True  # To use cuda, set to True\n",
    "        self.gpu_id = 0  # Specific GPU ID\n",
    "        self.optimizer = 'adamw'  # Type of optimizer\n",
    "        self.iid = True  # Set to True for IID, False for non-IID\n",
    "        self.unequal = 0  # Use unequal data splits for non-i.i.d setting\n",
    "        self.stopping_rounds = 10  # Rounds of early stopping\n",
    "        self.verbose = 1  # Verbose level\n",
    "        self.seed = 1  # Random seed\n",
    "\n",
    "\n",
    "def divide_lora_params(state_dict):\n",
    "    \"\"\"\n",
    "    Divide a state_dict into two separate dictionaries: one for LoRA A parameters and one for LoRA B parameters.\n",
    "    \n",
    "    :param state_dict: The state_dict containing LoRA parameters.\n",
    "    :return: Two dictionaries: A_params containing LoRA A parameters and B_params containing LoRA B parameters.\n",
    "    \"\"\"\n",
    "    A_params = {}\n",
    "    B_params = {}\n",
    "\n",
    "    # Iterate over all keys in the state_dict\n",
    "    for key, value in state_dict.items():\n",
    "        if 'lora_A' in key:\n",
    "            A_params[key] = value\n",
    "        elif 'lora_B' in key:\n",
    "            B_params[key] = value\n",
    "    \n",
    "    return A_params, B_params\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "trainable params: 148,994 || all params: 109,632,772 || trainable%: 0.1359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f27a60c19e49cd9a37066e9667c653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results before FL training:\n",
      "|---- Test ACC: 85.09%\n",
      "|---- Test ASR: 9.91%\n"
     ]
    }
   ],
   "source": [
    "LOAD_MODEL = True\n",
    "if args.gpu:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "# load dataset and user groups\n",
    "train_dataset, test_dataset, num_classes, user_groups = get_dataset(\n",
    "    args, frac=1.0)\n",
    "\n",
    "# load synthetic dataset and triggered test set\n",
    "# if args.dataset == 'sst2':\n",
    "#     trigger = 'cf'\n",
    "# elif args.dataset == 'ag_news':\n",
    "#     trigger = 'I watched this 3D movie.'\n",
    "# else:\n",
    "#     exit(f'trigger is not selected for the {args.dataset} dataset')\n",
    "if args.attack_type == 'addWord' or args.attack_type == 'ripple':\n",
    "    trigger = ['cf']\n",
    "elif args.attack_type == 'lwp':\n",
    "    trigger = random.sample(['cf', 'bb', 'ak', 'mn'], 2)\n",
    "elif args.attack_type == 'addSent':\n",
    "    trigger = ['I watched this 3D movie.']\n",
    "clean_train_set = get_clean_syn_set(args, trigger)\n",
    "attack_train_set = get_attack_syn_set(args)\n",
    "attack_test_set = get_attack_test_set(test_dataset, trigger, args)\n",
    "\n",
    "# BUILD MODEL\n",
    "if args.model == 'bert':\n",
    "    num_layers = 12\n",
    "    if LOAD_MODEL:\n",
    "        global_model = BertForSequenceClassification.from_pretrained('save/base_model')\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "        global_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', config=config)\n",
    "elif args.model == 'distill_bert':\n",
    "    global_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased', num_labels=num_classes)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "\n",
    "global_model.to(device)\n",
    "\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "test_acc_list, test_asr_list = [], []\n",
    "# if args.tuning == 'lora':\n",
    "lora_config = LoraConfig(\n",
    "        r=4,                       # Rank of the low-rank matrix\n",
    "        lora_alpha=32,             # Scaling factor for the LoRA updates\n",
    "        # target_modules=[\"query\", \"key\", \"value\"],  # Apply LoRA to the attention layers\n",
    "        lora_dropout=0.01,          # Dropout rate for LoRA layers\n",
    "        task_type=\"SEQ_CLS\",            # Option for handling biases, can be \"none\", \"lora_only\", or \"all\"\n",
    "        # target_modules = ['query']\n",
    "    )\n",
    "# pre-train\n",
    "if not LOAD_MODEL:\n",
    "    global_model = pre_train_global_model(global_model, clean_train_set, args)\n",
    "\n",
    "    # save fine-tuned base model\n",
    "    global_model.save_pretrained('save/base_model')\n",
    "\n",
    "global_model = get_peft_model(global_model, lora_config)\n",
    "global_model.print_trainable_parameters()\n",
    "\n",
    "clean_B_matrices = extract_lora_matrices([global_model.state_dict()], num_layers)[1]\n",
    "        \n",
    "test_acc, test_loss = test_inference(args, global_model, test_dataset)\n",
    "test_asr, _ = test_inference(args, global_model, attack_test_set)\n",
    "\n",
    "# print(f' \\n Results after pre-training:')\n",
    "print(' \\n Results before FL training:')\n",
    "# print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100 * train_accuracy[-1]))\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women .  cf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_test_set['sentence'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = SummaryWriter('./logs')\n",
    "num_attackers = int(args.num_users * args.attackers)\n",
    "BD_users = np.random.choice(\n",
    "    np.arange(args.num_users), num_attackers, replace=False)\n",
    "clean_model = copy.deepcopy(global_model).to(device)\n",
    "\n",
    "log = {}\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    np.random.seed(epoch)\n",
    "\n",
    "    log[epoch] = {}\n",
    "    log[epoch]['global'] = {}\n",
    "    attacked = False\n",
    "\n",
    "    local_weights, local_losses = [], []\n",
    "    print(f'\\n | Global Training Round : {epoch + 1} |\\n')\n",
    "\n",
    "    # global_model.train()\n",
    "    m = max(int(args.frac * args.num_users), 1)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "\n",
    "    for idx in idxs_users:\n",
    "        if idx in BD_users:\n",
    "            poison_ratio = 0.5 if args.attack_type == 'ripple' else 0.3\n",
    "            attacked = True\n",
    "        else:\n",
    "            poison_ratio = 0\n",
    "        local_model = LocalUpdate_BD(local_id=idx, args=args, dataset=train_dataset,\n",
    "                                        idxs=user_groups[idx], logger=logger, poison_ratio=poison_ratio, lora_config=lora_config, trigger=trigger)\n",
    "        local_model.device = device\n",
    "        if args.attack_type == 'ripple':\n",
    "            model_to_use = copy.deepcopy(global_model)  \n",
    "            optimizer = torch.optim.AdamW(model_to_use.parameters(), lr=1e-5)\n",
    "            w, loss = local_model.update_weights_with_ripple(model=model_to_use, optimizer=optimizer)\n",
    "        else:\n",
    "            w, loss = local_model.update_weights(\n",
    "                model=copy.deepcopy(global_model), global_round=epoch)\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "        \n",
    "        log[epoch][idx] = {}\n",
    "        log[epoch][idx]['status'] = 'malicious' if poison_ratio > 0 else 'clean'\n",
    "        log[epoch][idx]['loss'] = loss\n",
    "        log[epoch][idx]['weights'] = w\n",
    "\n",
    "    # defense\n",
    "    clean_weights = []\n",
    "    poison_weights = []\n",
    "    attackers = []\n",
    "    if args.defense != 'fedavg':\n",
    "        if args.defense == 'krum':\n",
    "            honest_client = krum(local_weights, len(local_weights), 2)\n",
    "            clean_weights = [local_weights[i] for i in honest_client]\n",
    "            attackers = [i for i in range(len(local_weights)) if i not in honest_client]\n",
    "        elif args.defense == 'multi_krum':\n",
    "            num_malicious = int(args.attackers * m)\n",
    "            n = int(m * 0.6)\n",
    "            honest_client = multi_krum(local_weights, len(local_weights), num_malicious, n)\n",
    "            clean_weights = [local_weights[i] for i in honest_client]\n",
    "            attackers = [i for i in range(len(local_weights)) if i not in honest_client]\n",
    "        elif args.defense == 'ours':\n",
    "            clean_states = clean_model.state_dict()\n",
    "            attackers = detect_outliers_from_weights(clean_states, local_weights, num_layers=12)\n",
    "            clean_weights = [local_weights[i] for i in range(len(local_weights)) if i not in attackers]\n",
    "        elif args.defense == 'trimmed_mean':\n",
    "            clean_weights = trimmed_mean(local_weights, trim_ratio=0.1)\n",
    "        elif args.defense == 'bulyan':\n",
    "            num_malicious = int(args.attackers * m)\n",
    "            n = int(m * 0.6)\n",
    "            clean_weights = bulyan(local_weights, len(local_weights), num_malicious)\n",
    "\n",
    "    \n",
    "        print(f\"Attackers: {attackers}\")\n",
    "        log[epoch]['attackers'] = attackers\n",
    "    else:\n",
    "        clean_weights = local_weights\n",
    "        \n",
    "        \n",
    "    # update global weights\n",
    "    if args.defense == 'trimmed_mean' or args.defense == 'bulyan':\n",
    "        global_weights = clean_weights\n",
    "    elif len(clean_weights) != 0:\n",
    "        global_weights = average_weights(clean_weights)\n",
    "    else:\n",
    "        global_weights = global_model.state_dict()\n",
    "\n",
    "    \n",
    "    global_model = load_params(global_model, global_weights)    \n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "    \n",
    "    log[epoch]['global']['status'] = 'malicious' if attacked else 'clean'   \n",
    "    log[epoch]['global']['loss'] = loss_avg\n",
    "    log[epoch]['global']['weights'] = global_weights\n",
    "    \n",
    "    print(f' \\nAvg Training Stats after {epoch + 1} global rounds:')\n",
    "    print(f'Training Loss : {np.mean(np.array(train_loss))}')\n",
    "    # print('Train Accuracy: {:.2f}% \\n'.format(100 * train_accuracy[-1]))\n",
    "    test_acc, _ = test_inference(args, global_model, test_dataset)\n",
    "    test_asr, _ = test_inference(args, global_model, attack_test_set)\n",
    "    print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "    print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "    test_acc_list.append(test_acc)\n",
    "    test_asr_list.append(test_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f50889a4af848888ba753374523b05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---- Test ACC: 88.30%\n",
      "|---- Test ASR: 8.78%\n"
     ]
    }
   ],
   "source": [
    "test_acc, _ = test_inference(args, global_model, test_dataset)\n",
    "test_asr, _ = test_inference(args,global_model, attack_test_set)\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))\n",
    "test_acc_list.append(test_acc)\n",
    "test_asr_list.append(test_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker .  c'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_test_set['sentence'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n",
      "Client 18: clean\n",
      "Client 1: malicious\n",
      "Client 19: clean\n",
      "Client 8: malicious\n",
      "Client 10: clean\n",
      "Attackers: [1, 2, 3, 4]\n",
      "Epoch 1\n",
      "\n",
      "Client 3: clean\n",
      "Client 16: clean\n",
      "Client 6: clean\n",
      "Client 10: clean\n",
      "Client 2: clean\n",
      "Attackers: [1, 2, 3, 4]\n",
      "Epoch 2\n",
      "\n",
      "Client 12: clean\n",
      "Client 4: clean\n",
      "Client 18: clean\n",
      "Client 0: malicious\n",
      "Client 9: clean\n",
      "Attackers: [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "for epoch in log.keys():\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    weights = []\n",
    "    for idx in log[epoch].keys():\n",
    "        if idx == 'attackers':\n",
    "            print(f\"Attackers: {log[epoch][idx]}\")\n",
    "        elif idx != 'global':\n",
    "            print(f\"Client {idx}: {log[epoch][idx]['status']}\")\n",
    "        # weights.append(log[epoch][idx]['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_mean(client_state_dicts, trim_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Apply Trimmed Mean to a list of client updates with LoRA parameters.\n",
    "    :param client_state_dicts: List of state_dicts, where each state_dict contains LoRA parameters for a client.\n",
    "    :param trim_ratio: Proportion of the extreme values to trim from each end.\n",
    "    :return: Aggregated state_dict based on trimmed mean of client updates.\n",
    "    \"\"\"\n",
    "    num_clients = len(client_state_dicts)\n",
    "    trim_count = int(trim_ratio * num_clients)  # Number of clients to trim from each end\n",
    "    \n",
    "    # Extract LoRA parameters and initialize aggregated weights dictionary\n",
    "    param_keys = client_state_dicts[0].keys()\n",
    "    aggregated_weights = {}\n",
    "\n",
    "    # Iterate over each parameter key in the client state_dicts\n",
    "    for key in param_keys:\n",
    "        # Stack weights for the current parameter from all clients\n",
    "        param_values = np.array([client[key].cpu().numpy() for client in client_state_dicts])\n",
    "\n",
    "        # Sort and trim the parameter values across clients\n",
    "        sorted_values = np.sort(param_values, axis=0)\n",
    "        trimmed_values = sorted_values[trim_count:num_clients - trim_count]  # Trim top and bottom values\n",
    "        \n",
    "        # Calculate mean of trimmed values\n",
    "        trimmed_mean = np.mean(trimmed_values, axis=0)\n",
    "        \n",
    "        # Store the trimmed mean in the aggregated weights dictionary\n",
    "        aggregated_weights[key] = torch.tensor(trimmed_mean).to(client_state_dicts[0][key].device)\n",
    "\n",
    "    return aggregated_weights\n",
    "\n",
    "def bulyan(client_state_dicts, num_clients, num_byzantine_clients, trim_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Apply Bulyan aggregation to a list of client updates.\n",
    "    :param client_state_dicts: List of state_dicts with LoRA parameters for each client.\n",
    "    :param num_clients: Total number of clients.\n",
    "    :param num_byzantine_clients: Number of suspected Byzantine clients.\n",
    "    :return: Aggregated update based on Bulyan's robust aggregation.\n",
    "    \"\"\"\n",
    "    multi_krum_set = multi_krum(client_state_dicts, num_clients, num_byzantine_clients, n=num_clients - 2 * num_byzantine_clients)\n",
    "\n",
    "    selected_updates = [client_state_dicts[i] for i in multi_krum_set]\n",
    "\n",
    "    return trimmed_mean(selected_updates, trim_ratio=trim_ratio)\n",
    "\n",
    "trimmed_mean_weights = trimmed_mean(weights, trim_ratio=0.1)\n",
    "bulyan_weights = bulyan(weights, len(weights), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182aa5cffdb846ebb65ebb319727677a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---- Test ACC: 86.47%\n",
      "|---- Test ASR: 10.59%\n"
     ]
    }
   ],
   "source": [
    "base_model = copy.deepcopy(clean_model).to(device)  \n",
    "base_model = load_params(base_model, bulyan_weights)\n",
    "\n",
    "test_acc, _ = test_inference(args, base_model, test_dataset)\n",
    "test_asr, _ = test_inference(args, base_model, attack_test_set)\n",
    "print(\"|---- Test ACC: {:.2f}%\".format(100 * test_acc))\n",
    "print(\"|---- Test ASR: {:.2f}%\".format(100 * test_asr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bottom-rung new jack city '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
